PS3: convolutional network training

In this problem set, you will explore how to auto-train a convolutional network.  A key
to this is deriving equivalent back-propagation derivatives for a convolution kernel.

A (toy) example is generated for this problem set, consisting of 28x28 gray-scale images.
One set of these images contains an 11-pixel wide white horizontal bar, with the remainder
of the pixels having random values in the range [0,1].  A second set of images also
corresponds to random intensity values of 28x28 images, but without the solid-white
horizontal bar.  We can say the white bar is a "feature" to be detected, and we
wish for a network to classify images with this feature as "1" and images without the
feature as "0".  These are the "target" values associated with the training images.

****make_training_data.m: (a "main" program)
The program "make_training_data.m" will create a file "cnn_data" that will
contain two matrices: one containing images with the feature ("images_w_feature"), and the 
other containing images without the feature ("images_wo_feature").  All images in the
first (3-D) matrix have an associated target value of "1", and all images in the
second matrix have a target value of "0."

Make_training_data.m can be run to create training data.  You can edit the image
dimensions, if desired, to evaluate or debug on smaller (or larger) images (by
editing the value NPIX).  You can also change the number of traning images to be 
created (by editing the value NIMAGES).

****manual_convnet() (a "main" program):
This is a main program.  It uses the functions W_conv_equiv(), convnet() and
convnet_equiv().  This program defines a single-hidden-layer convolutional network
with hard-coded values for the (first-layer) convolution kernel, biases for
both layers, and weights W2 from the hidden layer (featuremap) to the output
(a single node). The defined convolutional network is computed by the function
  [y_out ,featuremap_squashed]= convnet(test_image,kernel,bias_array,W2,y_bias);

 It is shown that this network is capable of classifying all of
the training images as containing or not containing the horizontal-bar feature.

Additionally, this program demonstrates that 2-D convolution is identical to
multiplication by some corresponding matrix.  For a given kernel and image size,
this matrix is computed by 
[W_conv,x1_dim,x2_dim] = W_conv_equiv(test_image,kernel);

This matrix is then used by the function convnet_equiv():
[y_squashed,featurevec_squashed]=convnet_equiv(test_image_SOH,W_conv,bias_vec1,W2,y_bias);
which simulates a conventional, single-hidden-layer, feedforward network.  It is
shown that convnet() and convnet_equiv() produce the same outputs.


***convnet():  a function to compute a convolutional network
function [y_squashed,x1,gprime1_vec,gprime2]=convnet(in_image,kernel,bias1_vec,W2,y_bias)
This function computes (simulates) a simple convolutional network.  It accepts
arguments of an input image (a matrix), and parameters of a convolutional network with
a single hidden layer.  The first layer performs convolution with the kernel "kernel"
(a matrix) operating on the input image.  The bias1_vec parameter is an additive bias
to each of the elements generated by the 2-D convolution operation.  These outputs
are then passed through a nonlinear activation function, implemented in "squash()",
which applies a logistic function (which saturates at 0 and at 1).  Squash() also
computes the slope (local derivative) of the activation function at the current operating
point.

The second layer is a fully-connected network, but with only a single output.  The
weights of this layer are described by the row vector "W2", with a scalar bias
of y_bias.  These components compute u2 = W2*x1+y_bias, and u2 is then passed through
the same squashing function.

Convnet() returns the resulting (scalar) output, y_squashed, as well
as the outputs of the hidden layer and the slopes of the activation functions at both
layers.

***convnet_equiv:  a function to compute a single-hidden-layer feedforward network
[y_squashed,featurevec_squashed] = convnet_equiv(test_image_SOH,W1,bias_vec1,W2,y_bias);
This function takes in a test image (expressed as a vector, strung-out horizontally),
and weight matrices W1 and W2 and biases bias_vec1 and y_bias, and it computes
the resulting output.  The activation functions are logistic (based on tanh()).  
This function is used to compare to convnet(), which shows that the computed matrix
W1 yields the same results as performing convolution with the specified kernel.

***squash(): a function to apply logistic activation functions
The activation function applied saturates at 0 and at 1 and is smooth and differentiable.
Activation is invoked on all elements of the provided argument.  Additionally,
the local derivatives, dx/du, are computed and returned for all elements of the
input argument.

***demo_gradient_tanh.m (a main program)
The program "demo_gradient_tanh.m" demonstrates an efficient computation of the
gradient of the logistic function (using tanh()).  This computation is incorporated in
squash().


****convnet_test_derivs.m: (a "main" program):
This program computes and validates sensitivities of network parameters for the
example single-hidden-layer convolutional network.  It uses the functions:
W_conv_equiv2(), convnet_equiv(), deltas_fnc(), compute_numerical_deriv_W2(),
compute_numerical_deriv_W1(), compute_numerical_deriv_kernel(), and squash().  It
reads in the cnn_data file for use in its computations.

The function W_conv_equiv2() computes a large, sparse matrix "Kvecs_map", which is
convenient for mapping 2-D kernels onto equivalent fully-connect synapse matrices.

Random parameters are generated for the convolutional network.  A computation of
the sensitivities of all kernel elements is performed, with the help of
Kvecs_map.  These sensitivities are compared (spot-checked) to perturbations
of individual kernel terms and the corresponding effects on the system error
(i.e., a numerical approximation of the derivative of E with respect to a chosen
kernel component).  The numerical approximation is performed by the function
compute_numerical_deriv_kernel(), which is simple but inefficient. The numerical
result is compared to the analytic computation to check for correctness.

The function "deltas_fnc()" computes the sensitivies of the bias terms for layer 1
(first layer) and layer 2, identical to a conventional fully-connected feedforward
network.  The W_sensitivities_fnc() computes the sensitivies of all synaptic weights
for both W1 and W2.  The function err_fnc() computes the penalty function for
the network output(s) vs the training-data target(s).  

The key element of this program is computation of the kernel sensitivities,
dEdK_analytic.  

!!!***********************************************

The version "convnet_test_derivs_incomplete.m" is missing the line:
%compute dE/dKij analytically:
dEdK_analytic = %USE W1_sensitivities and Kvecs_map to compute this

!!!***********************************************

Running the (complete) program should validate that all analytic sensitivites
calculated do agree with the corresponding numerical approximations.


****convnet_backprop.m (and convnet_backprop_incomplete.m) main program
This program uses the computation of sensitivities to perform back-propagation.
The learning-parameter (step size) "eta" is adjusted downwards if the
step is found to be too large (increasing error), and it is increased if the
step is found to improve the error.

convnet_backprop_incomplete.m is missing the formula for:
dEdK_analytic = %FIX ME!!!;

Every 100 iterations, this program saves its parameter data and plots out the
penalty results vs time.  

One should set the kernel size, e.g.:
 %choose dimensions of kernel:
 Krows=1 %3
Kcols= 11% 11

and the number of learning iterations: 
N_ITERS=20000 (needs to be larger)

The hope is that this backprop routine can discover a good set of weights for the
network, including the kernel coefficients.










